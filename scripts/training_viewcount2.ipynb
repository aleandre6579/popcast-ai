{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-17T03:46:40.877025Z",
     "iopub.status.busy": "2024-12-17T03:46:40.876106Z",
     "iopub.status.idle": "2024-12-17T03:46:40.887285Z",
     "shell.execute_reply": "2024-12-17T03:46:40.886083Z",
     "shell.execute_reply.started": "2024-12-17T03:46:40.876983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import random\n",
    "import librosa\n",
    "from torch.utils.data import Dataset\n",
    "from dotenv import load_dotenv\n",
    "from multiprocessing import Pool, cpu_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "DOWNLOAD_FOLDER = os.getenv(\"DOWNLOAD_FOLDER\")\n",
    "SAMPLE_RATE = 22050\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T03:46:40.890033Z",
     "iopub.status.busy": "2024-12-17T03:46:40.889664Z",
     "iopub.status.idle": "2024-12-17T03:46:45.231998Z",
     "shell.execute_reply": "2024-12-17T03:46:45.230778Z",
     "shell.execute_reply.started": "2024-12-17T03:46:40.889998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "songs_data = pd.read_csv('/mnt/f/Alex Stuff/songs_data_full.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T03:46:45.234221Z",
     "iopub.status.busy": "2024-12-17T03:46:45.233712Z",
     "iopub.status.idle": "2024-12-17T03:46:45.245588Z",
     "shell.execute_reply": "2024-12-17T03:46:45.244534Z",
     "shell.execute_reply.started": "2024-12-17T03:46:45.234141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, folder_path, sample_rate=22050, n_mels=128):\n",
    "        self.folder_path = folder_path\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.audio_files = random.sample([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.mp3')], 10)\n",
    "        self.spectrogram_batch_size = 500\n",
    "        self.spectrogram_batch = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_files[idx]\n",
    "        _, video_id, _ = self.audio_path_info(audio_path)\n",
    "        if len(songs_data[songs_data['videoID'] == video_id]) == 0:\n",
    "            idx += 1\n",
    "            self.__getitem__(idx)\n",
    "            return\n",
    "        \n",
    "        spectrogram = self.get_spectrogram_batch(idx)[idx]\n",
    "        views = torch.tensor(songs_data[songs_data['videoID'] == video_id]['views'].iloc[0], dtype=torch.float32)\n",
    "        return spectrogram.to(DEVICE), views.to(DEVICE)\n",
    "\n",
    "    def get_spectrogram_batch(self, current_idx):\n",
    "        if current_idx % self.spectrogram_batch_size != 0:\n",
    "            return self.spectrogram_batch\n",
    "        \n",
    "        self.spectrogram_batch = {}\n",
    "        audio_paths_batch = self.audio_files[current_idx:min(current_idx+self.spectrogram_batch_size, self.__len__())]\n",
    "        with Pool(processes=cpu_count()) as pool:\n",
    "            args = enumerate(audio_paths_batch, start=current_idx)\n",
    "            print(len(args[0]))\n",
    "            results = pool.map(self.get_spectrogram, args)\n",
    "        \n",
    "        for idx, spectrogram in results:\n",
    "            self.spectrogram_batch[idx] = spectrogram\n",
    "            print(f\"Super finished spec {idx}\")\n",
    "        print(f\"Finished batch, current index = {current_idx}\")\n",
    "        return self.spectrogram_batch\n",
    "\n",
    "    def get_spectrogram(self, args):\n",
    "        idx, audio_path = args\n",
    "        print(f\"Getting spec {idx}\")\n",
    "        audio, _ = librosa.load(audio_path, sr=self.sample_rate)\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio, sr=self.sample_rate, n_mels=self.n_mels)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        spectrogram = torch.tensor(mel_spec_db, dtype=torch.float32).transpose(0, 1)  # Shape: (time, n_mels)\n",
    "        print(f\"Finished spec {idx}\")\n",
    "        return idx, spectrogram\n",
    "\n",
    "    def audio_path_info(self, audio_path):\n",
    "        try:\n",
    "            filename = os.path.basename(audio_path)\n",
    "            parts = filename.split(\"^\")\n",
    "            if len(parts) != 3:\n",
    "                raise ValueError(\"Input text does not match the expected format 'index^videoID^songTitle'\")\n",
    "            index, videoID, songTitle = parts\n",
    "            return int(index), videoID, songTitle\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing text '{audio_path}': {e}\")\n",
    "            return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T03:46:45.248832Z",
     "iopub.status.busy": "2024-12-17T03:46:45.248383Z",
     "iopub.status.idle": "2024-12-17T03:46:45.269841Z",
     "shell.execute_reply": "2024-12-17T03:46:45.268474Z",
     "shell.execute_reply.started": "2024-12-17T03:46:45.248792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNRegression(\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# CNN Model for Song Embeddings\n",
    "class CNNRegression(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CNNRegression, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear((input_dim // 4) * 32, 128)  # Flattened size after pooling\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add channel dimension: (batch_size, 1, input_dim)\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = 128\n",
    "model_cnn = CNNRegression(input_dim)\n",
    "print(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T03:46:45.271901Z",
     "iopub.status.busy": "2024-12-17T03:46:45.271452Z",
     "iopub.status.idle": "2024-12-17T03:46:45.286951Z",
     "shell.execute_reply": "2024-12-17T03:46:45.285755Z",
     "shell.execute_reply.started": "2024-12-17T03:46:45.271853Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMRegression(\n",
      "  (lstm): LSTM(128, 64, num_layers=2, batch_first=True)\n",
      "  (fc1): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# LSTM Model for Song Embeddings\n",
    "class LSTMRegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=2):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add sequence length dimension: (batch_size, 1, input_dim)\n",
    "        _, (hn, _) = self.lstm(x)  # hn is the hidden state\n",
    "        x = self.relu(self.fc1(hn[-1]))  # Use the last hidden state\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model_lstm = LSTMRegression(input_dim)\n",
    "print(model_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T03:46:45.288823Z",
     "iopub.status.busy": "2024-12-17T03:46:45.288428Z",
     "iopub.status.idle": "2024-12-17T03:46:45.306342Z",
     "shell.execute_reply": "2024-12-17T03:46:45.305276Z",
     "shell.execute_reply.started": "2024-12-17T03:46:45.288789Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerRegression(\n",
      "  (embedding): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alean/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Transformer Model for Song Embeddings\n",
    "class TransformerRegression(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=4, dim_feedforward=128, num_layers=2):\n",
    "        super(TransformerRegression, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, input_dim)  # Input projection\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=num_heads, dim_feedforward=dim_feedforward)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # Add sequence length dimension: (batch_size, 1, input_dim)\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling over sequence\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model_transformer = TransformerRegression(input_dim)\n",
    "print(model_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size=128, hidden_size=128, num_layers=2):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Mel spectrogram (time_steps, n_mels)\n",
    "        \"\"\"\n",
    "        output, _ = self.rnn(x)\n",
    "        last_hidden_state = output[:, -1, :] # get last hidden step\n",
    "        out = self.fc(last_hidden_state)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RecursiveNN(nn.Module):\n",
    "    def __init__(self, input_size=128, hidden_size=128):\n",
    "        super(RecursiveNN, self).__init__()\n",
    "        self.fc_merge = nn.Linear(input_size * 2, hidden_size)\n",
    "        self.fc_output = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Recursively merges time steps.\n",
    "        Args:\n",
    "            x: Mel spectrogram (time_steps, n_mels)\n",
    "        \"\"\"\n",
    "        while x.size(0) > 1:\n",
    "            # Merge pairs of consecutive time steps\n",
    "            if x.size(0) % 2 == 1:  # Duplicate last step if odd number of steps\n",
    "                x = torch.cat((x, x[-1:]), dim=0)\n",
    "            x = x.view(x.size(0) // 2, -1)  # Pairwise merge\n",
    "            x = torch.relu(self.fc_merge(x))\n",
    "        \n",
    "        # Final output\n",
    "        out = self.fc_output(x.squeeze(0))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ZeroPaddingModel(nn.Module):\n",
    "    def __init__(self, input_size=128, hidden_size=128, num_layers=2):\n",
    "        super(ZeroPaddingModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Padded input (batch_size, max_time, n_mels)\n",
    "            lengths: Original lengths of each spectrogram\n",
    "        \"\"\"\n",
    "        packed_input = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (h_n, c_n) = self.lstm(packed_input)\n",
    "        last_hidden_states = h_n[-1]\n",
    "        out = self.fc(last_hidden_states)\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads spectrograms in a batch to the same length.\n",
    "    Args:\n",
    "        batch: List of (spectrogram, target) tuples.\n",
    "    Returns:\n",
    "        padded_spectrograms: Padded spectrogram tensor (batch_size, max_time, n_mels)\n",
    "        lengths: Lengths of each spectrogram (for RNN packing)\n",
    "        targets: View count targets\n",
    "    \"\"\"\n",
    "    spectrograms = [item[0] for item in batch]\n",
    "    targets = torch.stack([item[1] for item in batch])\n",
    "    \n",
    "    # Pad spectrograms to the same length\n",
    "    padded_spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True, padding_value=0.0)\n",
    "    return padded_spectrograms, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T03:46:45.308571Z",
     "iopub.status.busy": "2024-12-17T03:46:45.308096Z",
     "iopub.status.idle": "2024-12-17T03:46:45.322816Z",
     "shell.execute_reply": "2024-12-17T03:46:45.321590Z",
     "shell.execute_reply.started": "2024-12-17T03:46:45.308523Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_model(model, X, y, num_epochs=20, lr=0.001):\n",
    "    return\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Dummy input data\n",
    "X = torch.randn(1000, input_dim)\n",
    "y = torch.randn(1000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T03:46:45.325548Z",
     "iopub.status.busy": "2024-12-17T03:46:45.324523Z",
     "iopub.status.idle": "2024-12-17T03:46:46.420079Z",
     "shell.execute_reply": "2024-12-17T03:46:46.418883Z",
     "shell.execute_reply.started": "2024-12-17T03:46:45.325509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train CNN\n",
    "train_model(model_cnn, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T03:46:46.421913Z",
     "iopub.status.busy": "2024-12-17T03:46:46.421549Z",
     "iopub.status.idle": "2024-12-17T03:46:46.699312Z",
     "shell.execute_reply": "2024-12-17T03:46:46.698161Z",
     "shell.execute_reply.started": "2024-12-17T03:46:46.421878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train LSTM\n",
    "train_model(model_lstm, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T03:46:46.702343Z",
     "iopub.status.busy": "2024-12-17T03:46:46.701951Z",
     "iopub.status.idle": "2024-12-17T03:46:48.734601Z",
     "shell.execute_reply": "2024-12-17T03:46:48.733437Z",
     "shell.execute_reply.started": "2024-12-17T03:46:46.702307Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train Transformer\n",
    "train_model(model_transformer, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, dataloader, num_epochs=2, learning_rate=0.001, use_lengths=False):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for spectrograms, targets in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if use_lengths:  # Add zero padding\n",
    "                lengths = torch.tensor([spec.shape[0] for spec in spectrograms], dtype=torch.int64).to(DEVICE)\n",
    "                spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True, padding_value=0.0).to(DEVICE)\n",
    "                outputs = model(spectrograms, lengths)\n",
    "            else:\n",
    "                outputs = model(spectrograms)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "# Initialize models\n",
    "rnn_model = RNN().to(DEVICE)\n",
    "recursive_model = RecursiveNN().to(DEVICE)\n",
    "zero_padding_model = ZeroPaddingModel().to(DEVICE)\n",
    "\n",
    "# DataLoader\n",
    "folder_path = '/mnt/f/Alex Stuff/Songs'\n",
    "dataset = AudioDataset(folder_path)\n",
    "dataloader = DataLoader(dataset, batch_size=1)\n",
    "# For zero-padding model, use batch size > 1\n",
    "dataloader_padded = DataLoader(dataset, batch_size=8, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN Model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'enumerate' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[359], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining RNN Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[358], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, num_epochs, learning_rate, use_lengths)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     10\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mspectrograms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_lengths\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Add zero padding\u001b[39;49;00m\n",
      "File \u001b[0;32m~/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[346], line 21\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(idx)\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m spectrogram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_spectrogram_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m[idx]\n\u001b[1;32m     22\u001b[0m views \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(songs_data[songs_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideoID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m video_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviews\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spectrogram\u001b[38;5;241m.\u001b[39mto(DEVICE), views\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "Cell \u001b[0;32mIn[346], line 33\u001b[0m, in \u001b[0;36mAudioDataset.get_spectrogram_batch\u001b[0;34m(self, current_idx)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mcpu_count()) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m     32\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28menumerate\u001b[39m(audio_paths_batch, start\u001b[38;5;241m=\u001b[39mcurrent_idx)\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     34\u001b[0m     results \u001b[38;5;241m=\u001b[39m pool\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_spectrogram, args)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, spectrogram \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'enumerate' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(\"Training RNN Model...\")\n",
    "train_model(rnn_model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Recursive Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alean/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([8696, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/alean/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([15910, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/alean/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([5899, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/alean/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([8111, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/alean/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([10362, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/alean/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([20496, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/alean/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([7297, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/alean/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([6540, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/alean/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([13961, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/home/alean/apps/popcast-ai/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:608: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([9480, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 413253561477685.1250\n",
      "Epoch [2/2], Loss: 413253194831674.0625\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Recursive Model...\")\n",
    "train_model(recursive_model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Zero-Padding Model...\n",
      "Epoch [1/2], Loss: 259307412389888.0000\n",
      "Epoch [2/2], Loss: 259307412389888.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Zero-Padding Model...\")\n",
    "train_model(zero_padding_model, dataloader_padded, use_lengths=True)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6318567,
     "sourceId": 10221161,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
