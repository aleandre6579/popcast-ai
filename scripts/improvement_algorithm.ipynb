{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Similar Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import psycopg2\n",
    "from pgvector.psycopg2 import register_vector\n",
    "import tkinter as tk\n",
    "from tkinter import Button, Label\n",
    "import subprocess\n",
    "import multiprocessing as mp\n",
    "import json\n",
    "import librosa\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow logs\n",
    "import tensorflow as tf\n",
    "from essentia.standard import (\n",
    "    MonoLoader,\n",
    "    TensorflowPredict2D,\n",
    "    TensorflowPredictEffnetDiscogs,\n",
    "    TensorflowPredictVGGish,\n",
    "    Danceability,\n",
    "    Spectrum,\n",
    "    FrameCutter,\n",
    "    Loudness,\n",
    "    RhythmExtractor2013,\n",
    "    KeyExtractor,\n",
    "    Energy,\n",
    "    TonalExtractor,\n",
    "    Inharmonicity,\n",
    "    MFCC,\n",
    "    OnsetRate,\n",
    "    SpectralCentroidTime,\n",
    "    DynamicComplexity,\n",
    "    SpectralPeaks,\n",
    "    NoveltyCurve,\n",
    "    Spectrum,\n",
    "    BeatsLoudness,\n",
    "    Beatogram,\n",
    "    Meter,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "load_dotenv()\n",
    "AUDIO_DIR = os.getenv('DOWNLOAD_FOLDER')\n",
    "POSTGRES_PASSWORD = os.getenv('POSTGRES_PASSWORD')\n",
    "DATABASE_URL = f\"postgresql://user:{POSTGRES_PASSWORD}@45.149.206.230:5432/popcastdb\"\n",
    "CLAP_MODEL_PATH = \"./models/music_audioset_epoch_15_esc_90.14.pt\"\n",
    "DEVICE = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "DOWNLOAD_FOLDER = os.getenv('DOWNLOAD_FOLDER')\n",
    "CPU_THREADS = int(os.getenv(\"CPU_THREADS\"))\n",
    "MODELS_PATH = \"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_CONFIG = {\n",
    "    \"host\": '45.149.206.230',\n",
    "    \"dbname\": os.getenv('POSTGRES_DB'),\n",
    "    \"user\": os.getenv('POSTGRES_USER'),\n",
    "    \"password\": os.getenv('POSTGRES_PASSWORD'),\n",
    "    \"port\": 5432\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(**DB_CONFIG)\n",
    "register_vector(conn)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentia Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_essentia_algorithms(audio44k):\n",
    "    _, mfcc_coeffs = MFCC(inputSize=len(audio16k))(audio16k)\n",
    "    danceability_score = Danceability()(audio44k)\n",
    "    loudness_score = Loudness()(audio16k)\n",
    "    bpm, beat_positions, _, _, _ = RhythmExtractor2013(method=\"multifeature\")(audio44k)\n",
    "    key, scale, _ = KeyExtractor()(audio44k)\n",
    "    energy_score = Energy()(audio16k)\n",
    "\n",
    "    ### Chord Significances\n",
    "    _, _, _, _, chords, _, _, _, _, _, _, _ = TonalExtractor()(audio44k)\n",
    "    unique_chords, counts = np.unique(chords, return_counts=True)\n",
    "    chords_significance = {\n",
    "        chord: significance for (chord, significance) in zip(unique_chords, counts)\n",
    "    }\n",
    "\n",
    "    ### Inharmonicity\n",
    "    frames = []\n",
    "    frameCutter = FrameCutter()\n",
    "    while True:\n",
    "        frame = frameCutter(audio44k)\n",
    "        if not len(frame):\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    spectrum_magnitudes = []\n",
    "    for frame in frames:\n",
    "        spectrum_magnitudes_frame = Spectrum()(frame)\n",
    "        spectrum_magnitudes.append(spectrum_magnitudes_frame)\n",
    "    spectrum_magnitudes = np.array(spectrum_magnitudes).flatten()\n",
    "\n",
    "    frequencies, magnitudes = SpectralPeaks()(audio44k)\n",
    "    hnr_score = None\n",
    "    if len(frequencies) > 0 and frequencies[0]:\n",
    "        hnr_score = Inharmonicity()(frequencies, magnitudes)\n",
    "    ###\n",
    "\n",
    "    onset_rate_score = OnsetRate()(audio44k)\n",
    "    brightness_score = SpectralCentroidTime()(audio44k)\n",
    "    dynamic_complexity_score, _ = DynamicComplexity()(audio16k)\n",
    "\n",
    "    mel_bands = get_mel_bands(audio44k)\n",
    "    novelty_curve = NoveltyCurve()(mel_bands)\n",
    "    novelty_score = np.median(np.abs(np.diff(novelty_curve)))\n",
    "\n",
    "    beats_loudness, beats_loudness_band_ratio = BeatsLoudness(beats=beat_positions)(\n",
    "        audio44k\n",
    "    )\n",
    "    beatogram = Beatogram()(beats_loudness, beats_loudness_band_ratio)\n",
    "    time_signature = Meter()(beatogram)\n",
    "\n",
    "    features = {\n",
    "        \"Danceability\": danceability_score[0],\n",
    "        \"Loudness\": loudness_score,\n",
    "        \"BPM\": bpm,\n",
    "        \"Key\": key,\n",
    "        \"Key Scale\": scale,\n",
    "        \"Energy\": energy_score,\n",
    "        \"Chords Significance\": chords_significance,\n",
    "        \"Inharmonicity\": hnr_score,\n",
    "        \"Timbre\": np.mean(mfcc_coeffs),\n",
    "        \"Onset Rate\": onset_rate_score[1],\n",
    "        \"Brightness\": brightness_score,\n",
    "        \"Dynamic Complexity\": dynamic_complexity_score,\n",
    "        \"Novelty\": novelty_score,\n",
    "        \"Time Signature\": time_signature,\n",
    "    }\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Jamendo labels\n",
    "with open(\"data/mtg_jamendo_moodtheme-discogs-effnet-1.json\", \"r\") as jamendo_file:\n",
    "    jamendo_metadata = json.load(jamendo_file)\n",
    "jamendo_classes = jamendo_metadata[\"classes\"]\n",
    "\n",
    "with open(\"data/mtg_jamendo_instrument-discogs-effnet-1.json\", \"r\") as jamendo_file:\n",
    "    jamendo_instrument_metadata = json.load(jamendo_file)\n",
    "jamendo_instrument_classes = jamendo_instrument_metadata[\"classes\"]\n",
    "\n",
    "songs_data = pd.read_csv(\"data/songs_data.csv\", index_col=0)\n",
    "\n",
    "# Configure TensorFlow to use GPU efficiently\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Load Essentia models\n",
    "discogs_model = TensorflowPredictEffnetDiscogs(\n",
    "    graphFilename=MODELS_PATH + \"/discogs-effnet-bs64-1.pb\", output=\"PartitionedCall:1\"\n",
    ")\n",
    "vggish_model = TensorflowPredictVGGish(\n",
    "    graphFilename=MODELS_PATH + \"/audioset-vggish-3.pb\",\n",
    "    output=\"model/vggish/embeddings\",\n",
    ")\n",
    "approachability_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/approachability_regression-discogs-effnet-1.pb\",\n",
    "    output=\"model/Identity\",\n",
    ")\n",
    "engagement_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/engagement_regression-discogs-effnet-1.pb\",\n",
    "    output=\"model/Identity\",\n",
    ")\n",
    "arousal_valence_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/deam-audioset-vggish-2.pb\", output=\"model/Identity\"\n",
    ")\n",
    "aggressive_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/mood_aggressive-audioset-vggish-1.pb\",\n",
    "    output=\"model/Softmax\",\n",
    ")\n",
    "happy_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/mood_happy-audioset-vggish-1.pb\",\n",
    "    output=\"model/Softmax\",\n",
    ")\n",
    "party_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/mood_party-audioset-vggish-1.pb\",\n",
    "    output=\"model/Softmax\",\n",
    ")\n",
    "relaxed_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/mood_relaxed-audioset-vggish-1.pb\",\n",
    "    output=\"model/Softmax\",\n",
    ")\n",
    "sad_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/mood_sad-audioset-vggish-1.pb\", output=\"model/Softmax\"\n",
    ")\n",
    "jamendo_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/mtg_jamendo_moodtheme-discogs-effnet-1.pb\"\n",
    ")\n",
    "jamendo_instrument_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/mtg_jamendo_instrument-discogs-effnet-1.pb\"\n",
    ")\n",
    "acoustic_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/mood_acoustic-audioset-vggish-1.pb\",\n",
    "    output=\"model/Softmax\",\n",
    ")\n",
    "electronic_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/mood_electronic-audioset-vggish-1.pb\",\n",
    "    output=\"model/Softmax\",\n",
    ")\n",
    "voice_instrumental_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/voice_instrumental-audioset-vggish-1.pb\",\n",
    "    output=\"model/Softmax\",\n",
    ")\n",
    "gender_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/gender-audioset-vggish-1.pb\", output=\"model/Softmax\"\n",
    ")\n",
    "timbre_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/timbre-discogs-effnet-1.pb\", output=\"model/Softmax\"\n",
    ")\n",
    "reverb_model = TensorflowPredict2D(\n",
    "    graphFilename=MODELS_PATH + \"/nsynth_reverb-discogs-effnet-1.pb\",\n",
    "    output=\"model/Softmax\",\n",
    ")\n",
    "\n",
    "\n",
    "def run_essentia_models(audio44k):\n",
    "    features = {}\n",
    "\n",
    "    # Run models sequentially\n",
    "    discogs_embeddings = discogs_model(audio44k)\n",
    "    vggish_embeddings = vggish_model(audio44k)\n",
    "\n",
    "    # Process features sequentially\n",
    "    approachability = approachability_model(discogs_embeddings)\n",
    "    engagement = engagement_model(discogs_embeddings)\n",
    "    arousal_valence = arousal_valence_model(vggish_embeddings)\n",
    "    aggressive = aggressive_model(vggish_embeddings)\n",
    "    happy = happy_model(vggish_embeddings)\n",
    "    party = party_model(vggish_embeddings)\n",
    "    relaxed = relaxed_model(vggish_embeddings)\n",
    "    sad = sad_model(vggish_embeddings)\n",
    "    jamendo_labels = jamendo_model(discogs_embeddings)\n",
    "    jamendo_instruments = jamendo_instrument_model(discogs_embeddings)\n",
    "    acoustic = acoustic_model(vggish_embeddings)\n",
    "    electronic = electronic_model(vggish_embeddings)\n",
    "    voice_instrumental = voice_instrumental_model(vggish_embeddings)\n",
    "    gender = gender_model(vggish_embeddings)\n",
    "    timbre = timbre_model(discogs_embeddings)\n",
    "    reverb = reverb_model(discogs_embeddings)\n",
    "\n",
    "    # Process results into the features dictionary\n",
    "    features[\"Approachability\"] = np.median(np.squeeze(approachability))\n",
    "    features[\"Engagement\"] = np.median(np.squeeze(engagement))\n",
    "    arousal_valence_predictions = np.median(arousal_valence, axis=0)\n",
    "    features[\"Valence\"] = arousal_valence_predictions[0]\n",
    "    features[\"Arousal\"] = arousal_valence_predictions[1]\n",
    "    features[\"Aggressive\"] = np.median(aggressive, axis=0)[0]\n",
    "    features[\"Happy\"] = np.median(happy, axis=0)[0]\n",
    "    features[\"Party\"] = np.median(party, axis=0)[0]\n",
    "    features[\"Relaxed\"] = np.median(relaxed, axis=0)[0]\n",
    "    features[\"Sad\"] = np.median(sad, axis=0)[0]\n",
    "    jamendo_predictions = np.median(jamendo_labels, axis=0)\n",
    "    jamendo_dict = {\n",
    "        jamendo_class: jamendo_value\n",
    "        for jamendo_class, jamendo_value in zip(jamendo_classes, jamendo_predictions)\n",
    "    }\n",
    "    features[\"Jamendo Labels\"] = jamendo_dict\n",
    "    jamendo_instrument_predictions = np.median(jamendo_instruments, axis=0)\n",
    "    jamendo_instrument_dict = {\n",
    "        jamendo_class: jamendo_value\n",
    "        for jamendo_class, jamendo_value in zip(\n",
    "            jamendo_instrument_classes, jamendo_instrument_predictions\n",
    "        )\n",
    "    }\n",
    "    features[\"Jamendo Instruments\"] = jamendo_instrument_dict\n",
    "    features[\"Acoustic\"] = np.median(acoustic, axis=0)[0]\n",
    "    features[\"Electronic\"] = np.median(electronic, axis=0)[0]\n",
    "    voice_instrumental_predictions = np.median(voice_instrumental, axis=0)\n",
    "    features[\"Voice\"] = voice_instrumental_predictions[0]\n",
    "    features[\"Instrumental\"] = voice_instrumental_predictions[1]\n",
    "    gender_predictions = np.median(gender, axis=0)\n",
    "    features[\"Female\"] = gender_predictions[0]\n",
    "    features[\"Male\"] = gender_predictions[1]\n",
    "    timbre_predictions = np.median(timbre, axis=0)\n",
    "    features[\"Bright\"] = timbre_predictions[0]\n",
    "    features[\"Dark\"] = timbre_predictions[1]\n",
    "    reverb_predictions = np.median(reverb, axis=0)\n",
    "    features[\"Dry\"] = reverb_predictions[0]\n",
    "    features[\"Wet\"] = reverb_predictions[1]\n",
    "    features[\"Embeddings\"] = vggish_embeddings\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_file):\n",
    "    audio44k = MonoLoader(filename=audio_file)()\n",
    "    \n",
    "    algorithm_features = run_essentia_algorithms(audio44k)\n",
    "    model_features = run_essentia_models(audio44k)\n",
    "    \n",
    "    return algorithm_features | model_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_viewcount(video_id: str) -> int:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_songs(video_id: str) -> dict:\n",
    "    original_song_query = \"\"\"\n",
    "        SELECT video_id, filename \n",
    "        FROM audio_embeddings \n",
    "        WHERE video_id = %s;\n",
    "    \"\"\"\n",
    "    \n",
    "    similar_songs_query = \"\"\"\n",
    "        WITH target_embedding AS (\n",
    "            SELECT embedding \n",
    "            FROM audio_embeddings \n",
    "            WHERE video_id = %s\n",
    "        )\n",
    "        SELECT video_id, filename, \n",
    "            1 - (embedding <=> (SELECT embedding FROM target_embedding)) AS similarity\n",
    "        FROM audio_embeddings\n",
    "        WHERE video_id != %s\n",
    "        ORDER BY similarity DESC\n",
    "        LIMIT 10;\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Fetch original song details\n",
    "        cursor.execute(original_song_query, (video_id,))\n",
    "        original_song = cursor.fetchone()\n",
    "\n",
    "        # Fetch similar songs\n",
    "        cursor.execute(similar_songs_query, (video_id, video_id))\n",
    "        similar_songs = [{\"video_id\": row[0], \"filename\": row[1], \"similarity\": row[2]} for row in cursor.fetchall()]\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        return {\n",
    "            \"original\": {\"video_id\": original_song[0], \"filename\": original_song[1]},\n",
    "            \"similar_songs\": similar_songs\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(audio) -> dict:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_differences(user_audio, video_ids: list[str]) -> dict:\n",
    "    songs_query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM songs\n",
    "        WHERE video_id IN %s;\n",
    "    \"\"\"\n",
    "    cursor.execute(songs_query, (video_ids))\n",
    "    similar_songs = [{\"video_id\": row[0], \"filename\": row[1], \"similarity\": row[2]} for row in cursor.fetchall()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_audio(file_path):\n",
    "    try:\n",
    "        # Convert WSL path to Windows path\n",
    "        windows_path = file_path.replace(\"/mnt/\", \"\").replace(\"/\", \":\\\\\", 1).replace(\"/\", \"\\\\\")\n",
    "        \n",
    "        # Use full path to PowerShell for WSL compatibility\n",
    "        powershell_path = \"/mnt/c/Windows/System32/WindowsPowerShell/v1.0/powershell.exe\"\n",
    "        subprocess.run([powershell_path, \"Start-Process\", f\"'{windows_path}'\"], shell=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening audio: {e}\")\n",
    "\n",
    "def create_audio_player(song_data):\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Song Similarity Player\")\n",
    "\n",
    "    Label(root, text=\"Original Song\", font=(\"Helvetica\", 16)).pack(pady=10)\n",
    "\n",
    "    original_file_path = os.path.join(DOWNLOAD_FOLDER, song_data[\"original\"][\"filename\"])\n",
    "    Label(root, text=f\"Original: {song_data['original']['video_id']}\").pack()\n",
    "    Button(root, text=\"Play Original\", command=lambda: play_audio(original_file_path)).pack(pady=5)\n",
    "\n",
    "    Label(root, text=\"Top 10 Similar Songs\", font=(\"Helvetica\", 16)).pack(pady=10)\n",
    "\n",
    "    for idx, song in enumerate(song_data[\"similar_songs\"]):\n",
    "        song_label = f\"{idx+1}. {song['video_id']} (Similarity: {song['similarity']:.4f})\"\n",
    "        Label(root, text=song_label).pack()\n",
    "\n",
    "        file_path = os.path.join(DOWNLOAD_FOLDER, song[\"filename\"])\n",
    "        Button(root, text=\"Play\", command=lambda path=file_path: play_audio(path)).pack(pady=2)\n",
    "\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = \"b6o9cAzEFb0\"\n",
    "similar_songs = get_similar_songs(video_id)\n",
    "\n",
    "if similar_songs:\n",
    "    create_audio_player(similar_songs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
