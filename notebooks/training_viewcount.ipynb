{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "DOWNLOAD_FOLDER = os.getenv('DOWNLOAD_FOLDER')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Util Functions / Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load an MP3 file and convert it to a mel spectrogram\n",
    "def get_mel_spectrogram(filepath, sample_rate=22050, n_mels=128):\n",
    "    waveform, sr = torchaudio.load(filepath)\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to mel spectrogram\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mels=n_mels\n",
    "    )(waveform)\n",
    "    \n",
    "    # Normalize spectrogram\n",
    "    mel_spectrogram = (mel_spectrogram - mel_spectrogram.mean()) / (mel_spectrogram.std() + 1e-6)\n",
    "    \n",
    "    return mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class to load data and process mel spectrograms\n",
    "class SongDataset(Dataset):\n",
    "    def __init__(self, csv_path, sample_rate=22050, n_mels=128, duration=30):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.duration = duration\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Song filename follows this format: (index)^(video id)^(title).mp3 --> 0^LlWGt_84jpg^Special Breed.mp3 (example)\n",
    "        song_filename = str(idx) + '^' + self.data.iloc[idx]['videoID'] + '^' + self.data.iloc[idx]['title'] + '.mp3'\n",
    "        song_filepath = DOWNLOAD_FOLDER + '/' + song_filename\n",
    "        viewcount = self.data.iloc[idx]['views']\n",
    "\n",
    "        # Get the mel spectrogram\n",
    "        mel_spectrogram = get_mel_spectrogram(song_filepath, self.sample_rate, self.n_mels)\n",
    "        \n",
    "        return mel_spectrogram, torch.tensor(viewcount, dtype=torch.float32)\n",
    "    \n",
    "    # Slice the dataset using lower and upper bounds \n",
    "    def slice(self, lower: int, upper: int):\n",
    "        self.data = self.data.iloc[lower:upper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture with CNN + RNN\n",
    "class ViewCountPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViewCountPredictor, self).__init__()\n",
    "        \n",
    "        # CNN for feature extraction\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2))\n",
    "        )\n",
    "        \n",
    "        # RNN to handle variable-length sequences\n",
    "        self.rnn = nn.LSTM(input_size=128, hidden_size=64, num_layers=2, batch_first=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features with CNN\n",
    "        batch_size, channels, n_mels, time_steps = x.size()\n",
    "        x = x.view(batch_size, 1, n_mels, time_steps)  # Ensure single channel dimension for CNN\n",
    "        x = self.cnn(x)  # [batch_size, 128, new_n_mels, new_time_steps]\n",
    "        \n",
    "        # Flatten frequency dimension for RNN\n",
    "        x = x.mean(dim=2)  # Global average pooling across frequency dimension\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, time_steps, 128]\n",
    "        \n",
    "        # Pass through RNN\n",
    "        _, (hn, _) = self.rnn(x)\n",
    "        x = hn[-1]  # Take the last hidden state\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=20, lr=1e-3):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for mel_spectrogram, viewcount in train_loader:\n",
    "            mel_spectrogram = mel_spectrogram.unsqueeze(1)  # Add channel dimension\n",
    "            viewcount = viewcount.unsqueeze(1)  # Match output shape\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(mel_spectrogram)\n",
    "            loss = criterion(output, viewcount)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "        \n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for mel_spectrogram, viewcount in val_loader:\n",
    "                mel_spectrogram = mel_spectrogram.unsqueeze(1)\n",
    "                viewcount = viewcount.unsqueeze(1)\n",
    "                output = model(mel_spectrogram)\n",
    "                loss = criterion(output, viewcount)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEY: /mnt/d/Alex Stuff/Songs/9^0VseA79g9DE^Corporatist Utopia (Rap Version).mp3 0\n",
      "(1, 1)\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (3813881898.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[102], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    return\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "# Initializing Dataset and DataLoader\n",
    "dataset = SongDataset(csv_path='data/songs_data.csv')\n",
    "dataset.slice(0, 10)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = ViewCountPredictor()\n",
    "train_model(model, train_loader, test_loader, num_epochs=EPOCHS, lr=LEARNING_RATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
