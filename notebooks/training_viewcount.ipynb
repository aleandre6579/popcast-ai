{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "DOWNLOAD_FOLDER = os.getenv('DOWNLOAD_FOLDER')\n",
    "SAMPLE_RATE = 22050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Util Functions / Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load an MP3 file and convert it to a mel spectrogram\n",
    "def get_mel_spectrogram(filepath, sample_rate=22050, n_mels=128):\n",
    "    waveform, sr = torchaudio.load(filepath)\n",
    "    \n",
    "    # Resample if necessary\n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to mel spectrogram\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mels=n_mels\n",
    "    )(waveform)\n",
    "    \n",
    "    # Normalize spectrogram\n",
    "    mel_spectrogram = (mel_spectrogram - mel_spectrogram.mean()) / (mel_spectrogram.std() + 1e-6)\n",
    "    \n",
    "    return mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms audio signal to mel spectrogram\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    hop_length=512,\n",
    "    n_mels=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class to load data and process mel spectrograms\n",
    "class SongDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Song filename follows this format: (index)^(video id)^(title).mp3 --> 0^LlWGt_84jpg^Special Breed.mp3 (example)\n",
    "        song_filename = str(idx) + '^' + self.data.iloc[idx]['videoID'] + '^' + self.data.iloc[idx]['title'] + '.mp3'\n",
    "        song_filepath = DOWNLOAD_FOLDER + '/' + song_filename\n",
    "        viewcount = self.data.iloc[idx]['views']\n",
    "\n",
    "        # Get the mel spectrogram\n",
    "        signal, sr = torchaudio.load(song_filepath)\n",
    "        signal = self.resample(signal, sr)\n",
    "        signal = self.mix_down(signal)\n",
    "        signal = self.right_pad(signal)\n",
    "        signal = mel_spectrogram(signal)\n",
    "        \n",
    "        return signal, torch.tensor(viewcount, dtype=torch.float32)\n",
    "    \n",
    "    def resample(self, signal, sr):\n",
    "        if sr == SAMPLE_RATE:\n",
    "            return signal\n",
    "        resampler = torchaudio.transforms.Resample(sr, SAMPLE_RATE)\n",
    "        return resampler(signal)\n",
    "    \n",
    "    def mix_down(self, signal):\n",
    "        # Skip if signal is already Mono\n",
    "        if signal.shape[0] == 1:\n",
    "            return signal\n",
    "        # Convert signal shape from (2, 16000) -> (1, 16000)\n",
    "        return torch.mean(signal, dim=0, keepdim=True)\n",
    "    \n",
    "    def right_pad(self, signal):\n",
    "        signal_length = signal.shape[1]\n",
    "        if signal_length >= SAMPLE_RATE * 10 * 60: # Pad to 10 minutes\n",
    "            return signal\n",
    "        num_missing_samples = SAMPLE_RATE * 10 * 60 - signal_length\n",
    "        last_dim_padding = (0, num_missing_samples)\n",
    "        signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    # Slice the dataset using lower and upper bounds \n",
    "    def slice(self, lower: int, upper: int):\n",
    "        self.data = self.data.iloc[lower:upper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViewCountPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN for feature extraction\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2)),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 2))\n",
    "        )\n",
    "        \n",
    "        # RNN to handle variable-length sequences\n",
    "        self.rnn = nn.LSTM(input_size=128, hidden_size=64, num_layers=2, batch_first=True)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, 1, n_mels, time_steps]\n",
    "        lengths: list of original lengths before padding (for packing sequences)\n",
    "        \"\"\"\n",
    "        # Apply CNN to extract features\n",
    "        batch_size, channels, n_mels, time_steps = x.size()\n",
    "        x = x.view(batch_size, 1, n_mels, time_steps)  # Ensure single channel dimension for CNN\n",
    "        x = self.cnn(x)  # Output shape: [batch_size, 128, new_n_mels, new_time_steps]\n",
    "        \n",
    "        # Global average pooling across frequency dimension (n_mels -> 1)\n",
    "        x = x.mean(dim=2)  # Shape: [batch_size, 128, new_time_steps]\n",
    "\n",
    "        # Permute to match RNN input format: [batch_size, time_steps, features]\n",
    "        x = x.permute(0, 2, 1)  # Shape: [batch_size, new_time_steps, 128]\n",
    "        \n",
    "        # Pack the padded sequences for RNN processing\n",
    "        lengths = [sequence.size(2) for sequence in x]\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Pass through the RNN\n",
    "        packed_output, (hn, cn) = self.rnn(packed_input)\n",
    "        \n",
    "        # Use the last hidden state (from the last layer)\n",
    "        x = hn[-1]  # Shape: [batch_size, hidden_size]\n",
    "        \n",
    "        # Pass through the fully connected layers for final prediction\n",
    "        x = self.fc(x)  # Output shape: [batch_size, 1]\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, loss_fn, optimizer, device):\n",
    "    running_loss = 0.0\n",
    "    for mel_spectrogram, viewcount in data_loader:\n",
    "        mel_spectrogram, viewcount = mel_spectrogram.to(device), viewcount.to(device)\n",
    "        mel_spectrogram = mel_spectrogram.unsqueeze(1)\n",
    "        viewcount = viewcount.unsqueeze(1)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        output = model(mel_spectrogram)\n",
    "        loss = loss_fn(output, viewcount)\n",
    "            \n",
    "        # Perform backprop and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, loss_fn, optimizer, device, epochs=20, lr=1e-3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_one_epoch(model, train_loader, loss_fn, optimizer, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "        \n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for mel_spectrogram, viewcount in val_loader:\n",
    "                mel_spectrogram = mel_spectrogram.unsqueeze(1)\n",
    "                viewcount = viewcount.unsqueeze(1)\n",
    "                output = model(mel_spectrogram)\n",
    "                loss = loss_fn(output, viewcount)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f\"Validation Loss: {val_loss / len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     22\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[98], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, loss_fn, optimizer, device, epochs, lr)\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      4\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrunning_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Validation Step\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[97], line 9\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m viewcount \u001b[38;5;241m=\u001b[39m viewcount\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel_spectrogram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, viewcount)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Perform backprop and update weights\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ess/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ess/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[96], line 35\u001b[0m, in \u001b[0;36mViewCountPredictor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mx: [batch_size, 1, n_mels, time_steps]\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03mlengths: list of original lengths before padding (for packing sequences)\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Apply CNN to extract features\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m batch_size, channels, n_mels, time_steps \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     36\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m1\u001b[39m, n_mels, time_steps)  \u001b[38;5;66;03m# Ensure single channel dimension for CNN\u001b[39;00m\n\u001b[1;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn(x)  \u001b[38;5;66;03m# Output shape: [batch_size, 128, new_n_mels, new_time_steps]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# Initializing Datasets and DataLoaders\n",
    "dataset = SongDataset('data/songs_data.csv')\n",
    "dataset.slice(0, 10)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Initialize and train the model\n",
    "model = ViewCountPredictor().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train(model, train_loader, val_loader, loss_fn, optimizer, device, epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
