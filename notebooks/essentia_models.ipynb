{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c19fee",
   "metadata": {},
   "source": [
    "## Install/Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c24d98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "from dotenv import load_dotenv\n",
    "from tqdm.notebook import tqdm\n",
    "import psutil\n",
    "import tracemalloc\n",
    "import threading\n",
    "import json\n",
    "import h5py\n",
    "from essentia.standard import *\n",
    "#import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670504c7",
   "metadata": {},
   "source": [
    "#### Global Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "DOWNLOAD_FOLDER = os.getenv('DOWNLOAD_FOLDER')\n",
    "CPU_THREADS = int(os.getenv('CPU_THREADS'))\n",
    "MODELS_PATH = './models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3cf45",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c5935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classes for moodtheme predictor model\n",
    "with open('data/mtg_jamendo_moodtheme-discogs-effnet-1.json', 'r') as jamendo_file:\n",
    "    jamendo_metadata = json.load(jamendo_file)\n",
    "jamendo_classes = jamendo_metadata['classes']\n",
    "\n",
    "# Get classes for instrument predictor model\n",
    "with open('data/mtg_jamendo_instrument-discogs-effnet-1.json', 'r') as jamendo_file:\n",
    "    jamendo_instrument_metadata = json.load(jamendo_file)\n",
    "jamendo_instrument_classes = jamendo_instrument_metadata['classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1082ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_data = pd.read_csv('data/songs_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba2cb44",
   "metadata": {},
   "source": [
    "#### Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_memory_usage(process):\n",
    "    memory_summary = {f'Process {process.pid}': process.memory_info().rss / (1024 * 1024)}\n",
    "    for child in process.children(recursive=True):\n",
    "        memory_summary = memory_summary | {f'Child Process {child.pid}': child.memory_info().rss / (1024 * 1024)}\n",
    "    return memory_summary\n",
    "\n",
    "def print_memory_usage(process):\n",
    "    print(get_total_memory_usage(process))\n",
    "    snapshot = tracemalloc.take_snapshot()\n",
    "    print(f\"Top Consumer of Process {process.pid}: {snapshot.statistics('lineno')[0]}\")\n",
    "\n",
    "def monitor_memory_usage(process, kill_thread, interval=120):\n",
    "    while True:\n",
    "        try:\n",
    "            if kill_thread.value:\n",
    "                print(\"MONITOR THREAD KILLED\")\n",
    "                return\n",
    "            print_memory_usage(process)\n",
    "        except Exception as e:\n",
    "            print(f\"Thread ERROR: {e}\")\n",
    "            return\n",
    "        time.sleep(interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f582f3b",
   "metadata": {},
   "source": [
    "#### Extract Features Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61fb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_essentia_models(audio16k, audio44k):\n",
    "    features = {}\n",
    "    \n",
    "    # Get embeddings\n",
    "    discogs_embeddings = TensorflowPredictEffnetDiscogs(graphFilename=MODELS_PATH+'/discogs-effnet-bs64-1.pb', output=\"PartitionedCall:1\")(audio16k)\n",
    "    vggish_embeddings = TensorflowPredictVGGish(graphFilename=MODELS_PATH+'/audioset-vggish-3.pb', output=\"model/vggish/embeddings\")(audio16k)\n",
    "\n",
    "    # Approachability\n",
    "    approachability_predictions = TensorflowPredict2D(graphFilename=MODELS_PATH+'/approachability_regression-discogs-effnet-1.pb', output=\"model/Identity\")(discogs_embeddings)\n",
    "    approachability = np.median(np.squeeze(approachability_predictions))\n",
    "    \n",
    "    # Engagement\n",
    "    engagement_predictions = TensorflowPredict2D(graphFilename=MODELS_PATH+'/engagement_regression-discogs-effnet-1.pb', output=\"model/Identity\")(discogs_embeddings)\n",
    "    engagement = np.median(np.squeeze(engagement_predictions))\n",
    "    \n",
    "    # Arousal/Valence\n",
    "    arousal_valence_predictions = np.median(TensorflowPredict2D(graphFilename=MODELS_PATH+'/deam-audioset-vggish-2.pb', output=\"model/Identity\")(vggish_embeddings), axis=0)\n",
    "    valence = arousal_valence_predictions[0]\n",
    "    arousal = arousal_valence_predictions[1]\n",
    "    \n",
    "    # Aggressive\n",
    "    aggressive_predictions = TensorflowPredict2D(graphFilename=MODELS_PATH+'/mood_aggressive-audioset-vggish-1.pb', output=\"model/Softmax\")(vggish_embeddings)\n",
    "    aggressive = np.median(aggressive_predictions, axis=0)[0]\n",
    "    \n",
    "    # Happy\n",
    "    happy_predictions = TensorflowPredict2D(graphFilename=MODELS_PATH+'/mood_happy-audioset-vggish-1.pb', output=\"model/Softmax\")(vggish_embeddings)\n",
    "    happy = np.median(happy_predictions, axis=0)[0]\n",
    "    \n",
    "    # Party\n",
    "    party_predictions = TensorflowPredict2D(graphFilename=MODELS_PATH+'/mood_party-audioset-vggish-1.pb', output=\"model/Softmax\")(vggish_embeddings)\n",
    "    party = np.median(party_predictions, axis=0)[0]\n",
    "    \n",
    "    # Relaxed\n",
    "    relaxed_predictions = TensorflowPredict2D(graphFilename=MODELS_PATH+'/mood_relaxed-audioset-vggish-1.pb', output=\"model/Softmax\")(vggish_embeddings)\n",
    "    relaxed = np.median(relaxed_predictions, axis=0)[0]\n",
    "    \n",
    "    # Sad\n",
    "    sad_predictions = TensorflowPredict2D(graphFilename=MODELS_PATH+'/mood_sad-audioset-vggish-1.pb', output=\"model/Softmax\")(vggish_embeddings)\n",
    "    sad = np.median(sad_predictions, axis=0)[0]\n",
    "    \n",
    "    # Jamendo labels\n",
    "    jamendo_predictions = TensorflowPredict2D(graphFilename=MODELS_PATH+'/mtg_jamendo_moodtheme-discogs-effnet-1.pb')(discogs_embeddings)\n",
    "    jamendo_values = np.median(jamendo_predictions, axis=0)\n",
    "    jamendo_dict = {jamendo_class:jamendo_value for (jamendo_class, jamendo_value) in zip(jamendo_classes, jamendo_values)}\n",
    "    \n",
    "    # Jamendo instrument labels\n",
    "    jamendo_instrument_predictions = TensorflowPredict2D(graphFilename=MODELS_PATH+'/mtg_jamendo_instrument-discogs-effnet-1.pb')(discogs_embeddings)\n",
    "    jamendo_instrument_values = np.median(jamendo_instrument_predictions, axis=0)\n",
    "    jamendo_instrument_dict = {jamendo_class:jamendo_value for (jamendo_class, jamendo_value) in zip(jamendo_instrument_classes, jamendo_instrument_values)}\n",
    "    \n",
    "    # Acoustic\n",
    "    acoustic_predictions = TensorflowPredict2D(graphFilename=MODELS_PATH+'/mood_acoustic-audioset-vggish-1.pb', output=\"model/Softmax\")(vggish_embeddings)\n",
    "    acoustic = np.median(acoustic_predictions, axis=0)[0]\n",
    "    \n",
    "    # Electronic\n",
    "    electronic_predictions = TensorflowPredict2D(graphFilename=MODELS_PATH+'/mood_electronic-audioset-vggish-1.pb', output=\"model/Softmax\")(vggish_embeddings)\n",
    "    electronic = np.median(electronic_predictions, axis=0)[0]\n",
    "    \n",
    "    # Voice/Instrumental\n",
    "    voice_instrumental_predictions = np.median(TensorflowPredict2D(graphFilename=MODELS_PATH+'/voice_instrumental-audioset-vggish-1.pb', output=\"model/Softmax\")(vggish_embeddings), axis=0)\n",
    "    voice = voice_instrumental_predictions[0]\n",
    "    instrumental = voice_instrumental_predictions[1]\n",
    "    \n",
    "    # Gender (Male/Female)\n",
    "    gender_predictions = np.median(TensorflowPredict2D(graphFilename=MODELS_PATH+'/gender-audioset-vggish-1.pb', output=\"model/Softmax\")(vggish_embeddings), axis=0)\n",
    "    female = gender_predictions[0]\n",
    "    male = gender_predictions[1]\n",
    "    \n",
    "    # Timbre (Bright/Dark)\n",
    "    timbre_predictions = np.median(TensorflowPredict2D(graphFilename=MODELS_PATH+'/timbre-discogs-effnet-1.pb', output=\"model/Softmax\")(discogs_embeddings), axis=0)\n",
    "    bright = timbre_predictions[0]\n",
    "    dark = timbre_predictions[1]   \n",
    "    \n",
    "    # Reverb (Dry/Wet)\n",
    "    reverb_predictions = np.median(TensorflowPredict2D(graphFilename=MODELS_PATH+'/nsynth_reverb-discogs-effnet-1.pb', output=\"model/Softmax\")(discogs_embeddings), axis=0)\n",
    "    dry = reverb_predictions[0]\n",
    "    wet = reverb_predictions[1]\n",
    "    \n",
    "    # Return model results\n",
    "    features = {\n",
    "        'Embeddings': vggish_embeddings,\n",
    "        'Approachability': approachability,\n",
    "        'Engagement': engagement,\n",
    "        'Valence': valence,\n",
    "        'Arousal': arousal,\n",
    "        'Aggressive': aggressive,\n",
    "        'Happy': happy,\n",
    "        'Party': party,\n",
    "        'Relaxed': relaxed,\n",
    "        'Sad': sad,\n",
    "        'Jamendo Labels': jamendo_dict,\n",
    "        'Jamendo Instruments': jamendo_instrument_dict,\n",
    "        'Acoustic': acoustic,\n",
    "        'Electronic': electronic,\n",
    "        'Voice': voice,\n",
    "        'Instrumental': instrumental,\n",
    "        'Male': male,\n",
    "        'Female': female,\n",
    "        'Bright': bright,\n",
    "        'Dark': dark,\n",
    "        'Dry': dry,\n",
    "        'Wet': wet\n",
    "    }\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e934df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_file):\n",
    "    # Load the audio file\n",
    "    audio16k = MonoLoader(filename=audio_file, sampleRate=16000)()\n",
    "    audio44k = MonoLoader(filename=audio_file)()\n",
    "\n",
    "    # Run algorithms\n",
    "    algorithm_features = run_essentia_models(audio44k, audio16k)\n",
    "\n",
    "    # Merge results\n",
    "    return algorithm_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9236a2bf",
   "metadata": {},
   "source": [
    "#### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d1b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class constructed from song path\n",
    "# Song path must follow this format: /some/path/(int)^(video id)^(title).mp3\n",
    "#                               e.g  /some/path/0^LlWGt_84jpg^Special Breed.mp3\n",
    "class SongPath:\n",
    "    def __init__(self, song_path: str):\n",
    "        self.path = song_path\n",
    "        self.filename = os.path.basename(song_path)\n",
    "\n",
    "        song_filename_split = self.filename.split('^')\n",
    "        if len(song_filename_split) != 3:\n",
    "            raise Exception(\"The song's filename doesn't follow the correct format: /some/path/(int)^(video id)^(title).mp3\")\n",
    "        \n",
    "        self.index, self.video_id, self.title_with_extension = song_filename_split\n",
    "\n",
    "        self.index = int(self.index)\n",
    "        self.title = os.path.splitext(self.title_with_extension)[0]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Idx: {self.index},  videoID: {self.video_id}, title: {self.title_with_extension}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815d7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_song(song_path):\n",
    "    song = SongPath(song_path)\n",
    "    song_features = extract_audio_features(song.path)\n",
    "    return song.index, song_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46d0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_songs():\n",
    "    tracemalloc.start()\n",
    "    \n",
    "    song_paths = np.array([os.path.join(DOWNLOAD_FOLDER, song_filename) for song_filename in os.listdir(DOWNLOAD_FOLDER)])\n",
    "\n",
    "    #songs_data_lower, songs_data_higher = [len(song_paths)//6*0, len(song_paths)//6*1]\n",
    "    songs_data_lower, songs_data_higher = [0, 4]\n",
    "    song_paths = song_paths[songs_data_lower:songs_data_higher]\n",
    "    \n",
    "    hdf5_file_path = 'data/song_embeddings.h5'\n",
    "\n",
    "    with h5py.File(hdf5_file_path, 'w') as hdf5_file:\n",
    "        with mp.Manager() as manager:\n",
    "            kill_thread = manager.Value('b', False)\n",
    "\n",
    "            main_process = psutil.Process(os.getpid())\n",
    "            memory_thread = threading.Thread(target=monitor_memory_usage, args=(main_process, kill_thread))\n",
    "            memory_thread.start()\n",
    "            \n",
    "            song_results = []\n",
    "            for song_path in tqdm(song_paths, desc=\"Processing Songs\"):\n",
    "                processed_song = process_song(song_path)\n",
    "                song_results.append(processed_song)\n",
    "                        \n",
    "            kill_thread.value = True\n",
    "\n",
    "        # Aggregate results in the pandas dataframe\n",
    "        songs_data_full = songs_data.copy(deep=True)\n",
    "        for song_index, song_features in song_results:\n",
    "            for feature, value in song_features.items():\n",
    "                if feature == \"Embeddings\":\n",
    "                    song_name = os.path.splitext(os.path.basename(song_paths[song_index]))[0]\n",
    "                    hdf5_file.create_dataset(song_name, data=value, compression=\"gzip\")\n",
    "                    continue\n",
    "                \n",
    "                if feature not in songs_data_full.columns and isinstance(value, (tuple, set, list, np.ndarray, dict)):\n",
    "                    songs_data_full[feature] = np.nan\n",
    "                    songs_data_full[feature] = songs_data_full[feature].astype(object)\n",
    "                songs_data_full.at[song_index, feature] = value\n",
    "\n",
    "    return songs_data_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c37dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_by_index(index):\n",
    "    with h5py.File(hdf5_file_path, 'r') as hdf5_file:\n",
    "        # List all datasets in the HDF5 file (these are the song names)\n",
    "        song_names = list(hdf5_file.keys())\n",
    "        \n",
    "        # Get the song name by index (assuming index corresponds to the order of the datasets)\n",
    "        if index < len(song_names):\n",
    "            song_name = song_names[index]\n",
    "            \n",
    "            # Retrieve the embeddings for the selected song\n",
    "            embedding = hdf5_file[song_name][:]\n",
    "            print(\"Song: \", song_name)\n",
    "            return embedding\n",
    "        else:\n",
    "            print(f\"Index {index} is out of range. The file contains {len(song_names)} embeddings.\")\n",
    "            return None\n",
    "\n",
    "# Example usage\\\n",
    "hdf5_file_path = 'data/song_embeddings2.h5'\n",
    "index = 0  # The index of the song you want to retrieve\n",
    "embedding = get_embedding_by_index(index)\n",
    "\n",
    "if embedding is not None:\n",
    "    print(f\"Embedding for song at index {index}:\\n\", embedding)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5807643,
     "sourceId": 9535353,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5843102,
     "sourceId": 9582418,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 131168,
     "modelInstanceId": 106837,
     "sourceId": 126903,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 133887,
     "modelInstanceId": 109590,
     "sourceId": 130047,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 133915,
     "modelInstanceId": 109618,
     "sourceId": 130080,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30775,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
